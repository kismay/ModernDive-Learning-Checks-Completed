---
title: "Tidy Text Mining Learning"
author: "Karolyn Ismay <br> PhD Candidate, Pacific University"
date: "Summer 2019"
output: 
  html_document:
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

## turn on xaringan::inf_mr()

```

```{r load-packages, message=FALSE, results="hide", include=FALSE}

c(
  "blogdown", 
  "bookdown",
  "broom", 
  "dplyr",
  "evaluate", 
  "fivethirtyeight", 
  "ggplot2", 
  "ggraph", 
  "googledrive", 
  "grid", 
  "gridExtra", 
  "gutenbergr", 
  "igraph", 
  "janeaustenr", 
  "kableExtra", 
  "knitr", 
  "lattice", 
  "lazyeval", 
  "lubridate", 
  "markdown", 
  "moderndive", 
  "purrr", 
  "quanteda",
  "RColorBrewer", 
  "readr", 
  "reshape2",
  "rmarkdown", 
  "scales",
  "shiny",
  "stopwords",
  "stringr",
  "textdata",
  "tibble",
  "tidyr", 
  "tidyselect", 
  "tidytext", 
  "tidyverse",
  "tm",
  "widyr",
  "wordcloud", 
  "xaringan"
) -> package_vector

purrr::walk(.x = package_vector, .f = library, character.only = TRUE)

## turn on xaringan::inf_mr()

```

## Introduction

This page documents my completion of the practice examples in the [Tidy Text Mining](https://www.tidytextmining.com/). In addition, this document itself shows my learning and skill development with R.

## Chapter 1 Notes: Tidy text format

Tidy text format is defined as "a table with one-token-per-row" where a **token** is a "meaningful unit of text, such as a word" used for analysis. To **tokenize** is to split text into tokens. Tidy text mining typically uses single word tokens. Other methods of storying text for analysis include n-grams, sentences, and paragraphs. Using tidy text data, or one-token-per-row/one-word-per-row, allows us to use many "tidy" R tools and packages.

Other text mining approaches to storing text:

* **String:** multiple words, sentences, paragraphs, character vectors. 
* **Corpus:** raw strings with metadata and additional details.
* **Document-term matrix:** a "sparse matrix describing a collection". A collection could be a corpus (as above). One row per each document and one column for each term. Used for word counts and ***tf-idf*** (Chapter 3, "term frequency - inverse document frequency").

### 1.2 The 'unnest_tokens' function

Converting text into a tidy text dataset. 

1. Feed in the data,
2. Put it into a data frame,
3. Take data frame and ` %>% ` into `unnest_tokens()` function and specify `(output, input)` within the function,


Below is a poem shown as a character vector:
```{r unnest-tokens-1}
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")
text
```

Here we see the text in quotation marks and separated by commas. We've concatenated it with `c()` and assigned the data as `text` with `<-` operator.

Now, we need to put it into a data frame:
```{r unnest-tokens-2}
## Make sure `library(dplyr)` is loaded. `tibble()` is a tool in `dplyr`. `tibble` does not convert strings to factors and does not use row names, important for tidy tools.

text_df <- tibble(line = 1:4, text = text)

text_df
```

Here we see the data in a data frame, but the text is not in the tidy text format of one-token-per-document-per-row. We need to **tokenize** it!

```{r unnest-tokens-3}
## Make sure library(tidytext) is loaded.

text_df %>% 
  unnest_tokens(word, text)
```

The basic arguments in `unnest_tokens()` are the output (`word`) and the input (`text`, a column from `text_df`). This is reading in `text` column and unnesting the data by each word. We also see that `unnest_tokens()` includes a column with `line` for the line where each word occurs.

`unnest_tokens()` will convert the text/tokens to lowercase. To prevent this, feed in `to_lower = FALSE` arguement:
```{r unnest-tokens-4}
text_df %>% 
  unnest_tokens(word, text, to_lower = FALSE)
```

### 1.3 Tidying the works of Jane Austen
Clicking on `janeaustenr` in the Packages tab will display documentation for the package. For `janeaustenr`, this includes the various dataframes and the texts. The package has six novels where the text data is saved in a one-row-per-line format where a line is the literal printed line in a physical book. If we `View(emma)` we see the book Emma has been broken down line-by-line in one column. `View(austen_books)` shows two columns, one labeled `text` and the other `book`. `text` is again broken down line-by-line and and `book` holds the title of the book where that line is found. Also, we see rows with chapter numbers.

Since we want to tidy the data word-by-word, it would be useful create a new data frame that also includes columns as to where each word is found by line number and chapter. To do this, we will use `group_by()`, `mutate()`, and finally `ungroup()`:
```{r tidying-jane}
## load `janeaustenr`, `dplyr`, and `stringr` packages

original_books <- austen_books() %>% 
  group_by(book) %>% 
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>% 
  ungroup()

original_books
```

Here we have the data frame `original_books` which includes all the data from `austen_books` but also adds `linenumber` and `chapter` as columns.

Next, we nee to **tokenize** the text in a one-token-per-row format with `unnest_tokens()`:
```{r tidying-jane2}
tidy_books <- original_books %>% 
  unnest_tokens(word, text)
tidy_books
```

Now the data is in the tidy text data format.

We'll notice that on observation 10, the word "the" is present. The word "the" is considered a stop word, a word (such as *of, to, a*) typically not useful for analysis. `View(stop_words)` will show the list of stop words by lexicon. To remove the stop words, we can use `anti_join()`. To use only one particular lexicon, we can add `filter()`.
```{r tidying-jane3, message=FALSE}
data(stop_words)

tidy_books2 <- tidy_books %>% 
  anti_join(stop_words)
tidy_books2

## To filter:
## stop_words_onix <- stop_words %>% 
##  filter(lexicon == "onix")
## replace anti_join(stop_words) with: anti_join(stop_words_onix)
```

To find the most common words, we can use dplyr's `count()`:
```{r tidying-jane-count}
tidy_books2 %>% 
  count(word, sort = TRUE)
```

With the text data in the tidy format, we can create visualizations using `ggplot2`. Here we have a plot with the most frequent words:
```{r tidying-jane-plot}
tidy_books2 %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 600) %>% 
  mutate(word = reorder(word,n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

### 1.4 The gutenbergr package

The gutenbergr package can pull public domain works from the Project Gutenberg collection. The `gutenberg_download()` function can pull works by their Project Gutenberg ID. The package also has functions that can look at metadata, taking out unhelpful header/footer data, and look at information about authors.

`gutenberg_metadata` contains information about each work in the collection, such as the ID number of each work, the author, and which language it is in. To find texts we can use a variety of methods:
```{r find-texts1}
gutenberg_metadata %>% 
  filter(author == "Wells, H. G. (Herbert George)")

gutenberg_metadata %>% 
  filter(title == "The Time Machine")
```

### 1.5 Word frequencies

Let's download the texts for H. G. Wells' books *The Time Machine*, *The War of the Worlds*, *The Invisible Man*, and *The Island of Doctor Moreau*.
```{r download-hgwells, message=FALSE}

if(!file.exists("hgwells.rds")){
  hgwells <- gutenberg_download(c(35, 36, 5230, 159))
  write_rds(hgwells, "hgwells.rds")
} else{
  hgwells <- read_rds("hgwells.rds")
}


```

Next, we'll need to tidy the data:
```{r tidy-wells1, message=FALSE}
tidy_hgwells <- hgwells %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)
```

What are the most common words in these novels?
```{r most-common-hgwells}
tidy_hgwells %>% 
  count(word, sort = TRUE)
```

Next, we'll look at some works from the Brontë sisters: *Jane Eyre*, *Wuthering Heights*, *The Tenant of Wildfell Hall*, *Villette*, and *Agnes Grey*.
```{r bronte1}
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
```

Then we'll tidy them:
```{r bronte-tidy, message=FALSE}
tidy_bronte <- bronte %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words)
```

An now we'll look at the most common words:
```{r bronte-most-common}
tidy_bronte %>% 
  count(word, sort = TRUE)
```

Next, let's look at these three collections together. We'll calculate the fequency for each word in the three collections by binding the data frames together. We'll use `spread` and `gather` to reshape the dataframe so we can plot it.
```{r comparing-austen-hgwells-bronte}
frequency <- bind_rows(mutate(tidy_books2, author = "Jane Austen"),
                       mutate(tidy_hgwells, author = "H. G. Wells"),
                       mutate(tidy_bronte, author = "Brontë Sisters")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>% 
  count(author, word) %>% 
  group_by(author) %>% 
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>% 
  gather(author, proportion, `Brontë Sisters`:`H. G. Wells`)
```

We used `str_extract()` because the code from Project Gutenberg includes underscores to indicate emphasis such as italics. We want something like "_by_" to be counted the same as "by" instead of a whole other word.

Next, we'll plot:
```{r comparing-plot, message=FALSE, warning=FALSE}
ggplot(frequency,
       aes(x = proportion,
           y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position = "none") +
  labs(y = "Jane Austen", x = NULL)
```

This plot compares Austen's books to the Brontë Sisters and H. G. Wells. Words closer to the dashed line have simliar frequencies. Words further above the line are more often found in Austen's works. We see that Austen and the Brontë Sisters had closer frequencies of words than did Austen and H. G. Wells.

Next, we'll use a correlation test to see how similar or different these sets of word frequencies are.
```{r compare-freq-corr}
cor.test(data = frequency[frequency$author == "Brontë Sisters",],
         ~ proportion + `Jane Austen`)
cor.test(data = frequency[frequency$author == "H. G. Wells",],
         ~ proportion + `Jane Austen`)
```

Here we see that yes, there is more correlation between Austen's and the Brontë Sisters' works than between Austen's and Wells' works.

## Chapter 2 Notes: Sentiment analysis with tidy data

Sentiment analysis can look at individual words or the over sentiment of an entire work. There are many sentiment lexicons that look for positive/negative sentiment and also possibly emotions such as joy, anger, sadness, and so on. The `tidytext` package includes several lexicons including `AFINN`, `bing`, and `nrc`. 

To download these lexicons, we'll use `get_sentiments("")` and selection `1`.
```{r get-sentiments}

sentiments_afinn <- get_sentiments("afinn")
sentiments_bing <- get_sentiments("bing")
sentiments_nrc <- get_sentiments("nrc")

```



### 2.2 Sentiment analysis with inner join

Using `inner_join` for sentiment analysis is similar to using `anti_join`. To answer the question *what are the most common joy words in Emma*, we'll use `unnest_tokens()" again.
```{r emma-unnested}
tidy_books3 <- austen_books() %>% 
  group_by(book) %>% 
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>% 
  ungroup() %>% 
  unnest_tokens(word, text)
```

The output argument in `unnest_tokens()` is `word`. This is useful because the sentiment lexicons and the stop words datasets also have columns named `word`, thus making it easier to do inner and anti joins.

Now that the data is tidy, let's `filter()` the nrc lexicon for `joy` and join it with `tidy_books3` filtering for `Emma`.
```{r emma-joy, message=FALSE}
nrc_joy <- sentiments_nrc %>% 
  filter(sentiment == "joy")

tidy_books3 %>% 
  filter(book == "Emma") %>% 
  inner_join(nrc_joy) %>% 
  count(word, sort = TRUE)
```

Here we see a list of the most common "joy" words in the book Emma.

We can also look at sentiment changes over the course of a novel by defining chunks of text. Using too small of a chunk can lead to not having enough data to get a good idea of the sentiment while having too large of a chunk can water down the sentiment. It depends on each individual text. For this, we'll use 80 lines. We'll use `spread()` to create separate columns for negative and positive sentiment. We'll also calculate net sentiment.
```{r austen-sentiment-80lines, message=FALSE}

austen_sentiment80 <- tidy_books %>% 
  inner_join(sentiments_bing) %>% 
  count(book, index = linenumber %/% 80, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)

```

This gives us a data frame with the title of the book, and index number for the chunk of text we're looking at, the total negative and total positive words, and the net sentiment between the two.

Now we can plot `austen_sentiment80` with `index` on the x axis to show sentiment across the narrative timeline of each book. Since each book is a different length, we'll use `scales = "free_x"` in `facet_wrap()` to make the plots uniformly sized.
```{r austen-sentiment-80-plotted}
ggplot(austen_sentiment80, 
       aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

Having read a few of the books plotted, I have a good guess at what is going on in the plot when there are sudden changes in the net sentiment between chunks.

### 2.3 Comparing the three sentiment dictionaries

Some sentiment lexicons are more appropriate for some questions than others. Sometimes they're tailor made for certain purposes or they're made by people from a particular field to suite that field. Because of this, it can be useful to compare lexicons. Let's look at the book *Pride & Prejudice* with three different lexicons.

```{r pp3lexicons}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")
pride_prejudice
```

Now that we have the book isolated, we'll `inner_join` it with the lexicons. However, AFINN uses scores rather than positive/negative, so we'll need to treat that one separately. 
```{r pp3lexicons2, message=FALSE}
pp_afinn <- pride_prejudice %>% 
  inner_join(sentiments_afinn) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

pp_bing_nrc <- bind_rows(pride_prejudice %>% 
                           inner_join(sentiments_bing) %>% 
                           mutate(method = "Bing et al."),
                         pride_prejudice %>% 
                           inner_join(sentiments_nrc %>% 
                                        filter(sentiment %in% c("positive", "negative"))) %>% 
                           mutate(method = "NRC")) %>% 
  count(method, index = linenumber %/% 80, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)
  
```

Here we have the sentiment scores for each chunck of *Pride & Prejudice* in two data frames named `pp_afinn` and `pp_bing_nrc`. 

Now to plot them:
```{r pp3lexicons3}
bind_rows(pp_afinn, pp_bing_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol=1)
  
```

There are definite differences in the plots above, but they all follow the same general flow. The AFINN lexicon has higher highs, the NRC lexicon has very few negatives, and the Bing et al. lexicon tends to stay closer to 0. This shows us that each lexicon is unique. Let's take a look at the lexicons themselves.

Let's look at the number of negative and positive words in NRC and Bing et al.:
```{r compare-nrc-bing}
sentiments_nrc %>% 
  filter(sentiment %in% c("positive",
                          "negative")) %>% 
  count(sentiment)

sentiments_bing %>% 
  count(sentiment)
```

It looks like NRC has about a thousand more negative sentiment words than positive sentiment words. Bing et al. has 2776 more negative words than positive words. This could be making a difference in the *Pride & Prejudice* plots above.

### 2.4 Most common positive and negative words

Linking the texts with sentiments allows us to look at the most commong positive and negative words are. Let's look at Austen's works using the Bing et al. lexicon.
```{r mostcommon1, message=FALSE}
bing_word_counts <- tidy_books %>% 
  inner_join(sentiments_bing) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()
bing_word_counts

bing_word_counts %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y="contribution to sentiment", 
       x= NULL) +
  coord_flip()
```

Oh no! The word "miss" is marked as negative, as in "miss the mark" or "I miss my dog" when likely Austen used it as a title for characters such as "Hello Miss Emma". We can add the word "miss" to a set of custom stop words.

```{r custom-stop-words}
custom_stop_words <- bind_rows(tibble(word = c("miss"), lexicon = c("custom")),
                               stop_words)
custom_stop_words
```

Now let's plot number of positive and negative words without stop words:
```{r mostcommon-custom-stop-words, message=FALSE}
bing_word_counts <- tidy_books %>% 
  anti_join(custom_stop_words) %>% 
  inner_join(sentiments_bing) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()
bing_word_counts

bing_word_counts %>% 
  group_by(sentiment) %>% 
  top_n(10) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y="contribution to sentiment", 
       x= NULL) +
  coord_flip()
```

### 2.5 Wordclouds

The tidy text format works well for ggplot2, but it also works well for wordclouds. Let's try it:
```{r wordcloud1, message=FALSE}
tidy_books %>% 
  anti_join(stop_words) %>% 
  count(word) %>% 
  with(wordcloud(word, n, max.words = 100))

```

To do a `comparison.cloud()` we need to reshape the data frame to a matrix with `acast()`.
```{r comparison-cloud, message=FALSE}
tidy_books %>% 
  inner_join(sentiments_bing) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

### 2.6 Looking at unites beyond just words

Looking at singular words is useful, but there are times when looking at an entire sentence is more useful. Let's look at one:
```{r pp-sentence1, message=FALSE}
pp_sentences <- tibble(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")

pp_sentences$sentence[2]
```

Sentence tokenizing doesn't always work well with dialogue. Using `iconv()` for example `iconv(text, to = 'latin1')` in a mutate function before unnesting might be helpful. Another option is to use the regex pattern in `unnest_tokens()`.
```{r austen-by-chapters}

austen_chapters <- austen_books() %>% 
  group_by(book) %>% 
  unnest_tokens(chapter, text, token = "regex",
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>% 
  ungroup()
austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())
```

Let's find the most negative chapters in each of Austen's books. First we'll get the list of negative words from the Bing et al. lexicon. Then we'll make a data frame with the number of words in each chapter to normalize for length. Then we'll count the number of negative words in each chapter and divide by the total number of words in each chapter. Which chapter in each book as the highest proportion of negative words?
```{r most-negative-chapters, message=FALSE}
bing_neg <- sentiments_bing %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>% 
  group_by(book, chapter) %>% 
  summarise(words = n())

tidy_books %>% 
  semi_join(bing_neg) %>% 
  group_by(book, chapter) %>% 
  summarise(negativewords = n()) %>% 
  left_join(wordcounts, by = c("book", "chapter")) %>% 
  mutate(ratio = negativewords/words) %>% 
  filter(chapter !=0) %>% 
  top_n(1) %>% 
  ungroup()

```

## Chapter 3 Notes: Analyzing word and document frequency: tf-idf

Text mining aims to quantify what a document is about by looking at the words that make up that document. One way to do this is *term frequency* (tf), or how often a word shows up in a document. However, there are a lot of words that don't carry a lot of context, our stop words. However, sometimes there are words that are considered stop words that have more meaning than they seem. How do we account for that?

One way is to consider a word's *inverse document frequency* (idf). The idf decreases the weight of often used words and increases the weight of words that aren't used as often. Together, *tf-idf* can tell us the term's frequency adjusted for how often it appears in a document compared to a collection (corpus) of documents.

### 3.1 Term frequency in Jane Austen's novels

First, let's count the most common words. Use `dplyr`'s `group_by()` and `join()` functions.
```{r austen-frequency1}

book_words <- austen_books() %>%
  unnest_tokens(word, text) %>% 
  count(book, word, sort = TRUE)

total_words <- book_words %>% 
  group_by(book) %>% 
  summarise(total = sum(n))

book_words2 <- left_join(book_words, total_words, by = "book")

book_words2
```

Here `n` is the number of times that the `word` shows up in the `book` and the `total` is the total number of words in that `book`. Next, let's plot `n/total` for each novel.

```{r austen-freq-ntotal, message=FALSE, warning=FALSE}
ggplot(book_words2,
       aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, .0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y")
```

Here we see that there a few words that occur frequently and many words that occur less frequently.

### 3.2 Zipf's Law

Having distributions like the one above is very common in language and follows Zipf's Law that the frequency that word appears in a document is inversely proportional to its rank.

```{r zipfs1}
freq_by_rank <- book_words2 %>% 
  group_by(book) %>%
  mutate(rank = row_number(),
         'term frequency' = n/total)
freq_by_rank
```

Then we can plot `rank` and `term frequency`.
```{r zipfs2}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book)) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()
```

This plot shows that Austen's books tend to follow Zipf's law, but not perfectly. 

To find the exponent of the power law is for the middle section of the rank range.
```{r zipfs-power-law-exponent}
rank_subset <- freq_by_rank %>% 
  filter(rank < 500,
         rank > 10)

lm(log10(`term frequency`) ~ log10(rank), data = rank_subset)
```

Now to plot it:
```{r zipfs-plot}
freq_by_rank %>% 
  ggplot(aes(rank, `term frequency`, color = book )) +
  geom_abline(intercept = -0.62, slope = -1.1, color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) +
  scale_x_log10() +
  scale_y_log10()
```

It is common to see deviations where the high rank words are but it is apparently uncommon to see such deviation in the low rank words. Austen uses a lower percentage of the most common words.

### 3.3 The `bind_tf_idf` function

The `bind_tf_idf` function is part of the `tidytext` package. It takes a data frame with tidy text (one row per term per document). The arguments are as follows: `bind_tf_idf(tbl, term, document, n)` where `tbl` is a tidy text dataset, `term` is the column containing terms as string or symbol, `document` is the column containing the document IDs, as string or symbol, and `n` is the column containing document-term counts as string or symbol. Run `?bind_tf_idf` after you've loaded the `tidytext` package.

Let's use it:
```{r bind_tf_idf1}
book_words_bindtf <- book_words %>% 
  bind_tf_idf(word, book, n)

## notice here that we did not specify the `tbl` because we already said we're using the `book_words` data frame.

book_words_bindtf
```

Here we see that the most common words (the, two, and, of, etc.) have an idf of 0 and a tf-idf of essentially zero.

Let's look at terms that have high tf-idf:
```{r high-tf-idf}
 book_words_bindtf %>% 
  arrange(desc(tf_idf))
```

Here we see names of characters from the books. They are important and distictive of each book.

Now let's visualize the data:
```{r high-tf-idf-vis}
book_words_bindtf %>% 
  arrange(desc(tf_idf)) %>% 
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(book) %>% 
  top_n(15) %>% 
  ungroup() %>% 
  ggplot(aes(word, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~book, ncol = 2, scales = "free") +
  coord_flip()
```

Here I see the highest tf-idf words in each book. I see lots of character's names, meaning that Austen has similar language across her books and that names are distinctive to each book. I also notice that some of the names are repeated with an apostrophe-s, or the possessive of a name. 

### 3.4 A corpus of physics texts

Now we'll look at terms in a collection of classic physics books from Project Gutenberg: *Discourse on Floating Bodies* by Galileo Galilei, *Treatise on Light* by Christiaan Huygens, *Experiments with Alternate Currents of High Potential and High Frequency* by Nikola Tesla, and *Relativity: The Special and General Theory* by Albert Einstein.

I'll use the function `gutenberg_download()` from the `gutenbergr` package. Because these are all from different authors, we'll add the argument `meta_fields = "author"` to make sure it is included in the data frame.
```{r physics-download}
physics <- gutenberg_download(c(37729, 14725, 13476, 30155), 
                              meta_fields = "author")

## Next we'll tidy the data

physics_words <- physics %>% 
  unnest_tokens(word, text) %>% 
  count(author, word, sort = TRUE)
## in `count(author, word, sort = TRUE)` the `count` function is grouping by author, then by word, sorting in descending order.
physics_words
```

We see lots of stop words here. This is very common. When we calculate te tf-idf, we'll see which words are most important. One thing we'll need to be careful of is that each of these documents are different lengths. Remember the arguments for `bind_tf_idf(tbl, term, document, n)`. In this case, the term is still `word` but the document is `author`.

```{r physics-lengths}
library(forcats)

physics_tfidf <- physics_words %>% 
  bind_tf_idf(word, author, n) %>% 
  mutate(word = fct_reorder(word, tf_idf)) %>% 
  mutate(author = factor(author, levels = c("Galilei, Galileo", 
                                            "Huygens, Christiaan",
                                            "Tesla, Nikola", 
                                            "Einstein, Albert")))
physics_tfidf
```

Now, let's plot it:
```{r physics-plot}
physics_tfidf %>% 
  group_by(author) %>% 
  top_n(15, tf_idf) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, tf_idf)) %>% 
  ggplot(aes(word, tf_idf, fill = author)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf_idf") +
  facet_wrap(~author, ncol = 2, scales = "free") +
  coord_flip()
```

This is interesting but what's the deal with the "_" and the "_k_" and why was Tesla talking about figs so much? Let's take a look at the data:
```{r physics_stopwords1}
physics %>% 
  filter(str_detect(text, "_k_")) %>% 
  select(text)
```

This doesn't look too meaningful. I will add it to a group of stopwords to remove them:
```{r physics_stopwords2}
physics_stopwords <- tibble(word = c("ab", "ac", "rc", "cm", "cg", "_k_", "_k", "co", "_x", "fig", "cb", "ak", "d", "bn", "ad", "_v_", "cd", "_i.e", "bc"))

## When I ran all of this code, there were additional stopwords that appeared, so I came back to this section to add them. The words I added were "cb", "ak", "d", "bn", "ad", "_v_", "cd", "_i.e", and "bc".

physics_words_stopped <- anti_join(physics_words, physics_stopwords, by = "word")

## Now let's calculate the tf-idf and then plot the data:
physics_tfidf2 <- physics_words_stopped %>% 
  bind_tf_idf(word, author, n) %>% 
  mutate(word = str_remove_all(word, "_")) %>% 
  mutate(word = fct_reorder(word, tf_idf)) %>% 
  mutate(author = factor(author, levels = c("Galilei, Galileo", 
                                            "Huygens, Christiaan",
                                            "Tesla, Nikola", 
                                            "Einstein, Albert")))

## Notice we also removed the "_" we found.

physics_tfidf2


## Plotting:

physics_tfidf2 %>% 
  group_by(author) %>% 
  top_n(15, tf_idf) %>% 
  ungroup() %>% 
  mutate(word = reorder(word, tf_idf)) %>% 
  ggplot(aes(word, tf_idf, fill = author)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf_idf") +
  facet_wrap(~author, ncol = 2, scales = "free") +
  coord_flip()
```

Now the plot has words that are on their face much more meaningful than the plot we had before. If you compare my code with what's in tidytextmining, you'll see I plotted this a little differently. I noticed when I used `reorder_within()` and viewed the resulting data frame my text was no longer tidy. It had each word as "the_Einstein, Albert" and "the_Tesla, Nikola". I decided to use the code that was in the original bind&plot.

## Chapter 4 Notes: Relationships between words: n-grams and correlations

Tidy text has focused on one word per row per document, but now I'll look at n-grams and correlations. This will allow me to look at the relationships between words, words that follow other words and words that tend to show up nearby one another in documents. I'll use `token = "ngrams"` argument to look at adjacent words, `ggraph` to make network plots, and `widyr` that can "calculate pairwise correlations and distances within a tidy data frame".

### 4.1 Tokenizing by n-gram

`unnest_tokens()` can be adjusted by the argument `token = "ngrams"` and `n = 2` to set the number of words to 2. This will pull words that are directly next to one another, so we'll see overlap where word #2 is paired with word #1 and word #3. Each pair will have its own row. Thus row 1 would have words 1&2, row 2 would have words 2&3, row 3 would have words 3&4, and so on.

```{r bigrams1}
austen_bigrams <- austen_books() %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2)
austen_bigrams
```

### 4.1.1 Counting and filtering n-grams

`dplyr` can be used on n-grams:

```{r dplyr_bigrams}
austen_bigrams %>% 
  count(bigram, sort = TRUE)
```

There are lots of stop words here. Using `separate()` can split a column into multiple columns like so:

```{r separate_bigrams}
bigrams_separated <- austen_bigrams %>% 
  separate(bigram, c("word1", "word2"), sep = " ")
bigrams_separated

## Now we'll filter out the stop words:

bigrams_filtered <- bigrams_separated %>% 
  filter(!word1 %in% stop_words$word) %>% 
  filter(!word2 %in% stop_words$word)
bigrams_filtered

## Now to count:
bigrams_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)
bigrams_counts
```

Here we see that Austen often attaches titles to character's names. Next we can recombine the words with `unite()` from the `tidyr` package.

```{r unite_bigrams}
bigrams_united <- bigrams_filtered %>% 
  unite(bigram, word1, word2, sep = " ")
bigrams_united
```

Let's try it with trigams:

```{r austen_trigrams}
austen_trigams <- austen_books() %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>% 
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word, 
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>% 
  count(word1, word2, word3, sort = TRUE)

## the filter() is used with ! and %in% here. We're asking R to spit out all the words in word1 that are not also found in the stop_words data frame under the variable "word". When I think of filtering something I'm often thinking of "filtering out" or removing something, but I should think of it as a colander where the stuff that I want (like pasta) is what gets held on to, not the pasta water. 
## the ! operator was a little confusing here too and it's possibly due to the order. Initially I read it as "throw away the words that aren't in word1 in stop_words". However, to put the code into plain english, I should read it as "show me all the words in word1 that aren't also in stop_words' word column. I often feel like R writes everything backwards...

austen_trigams
```

### 4.1.2 Analyzing bigrams

Using bigrams we can look specifically at things that tend to go together in language, such as streets. We often write streets as "Yahmill street" or "Lovejoy street".

```{r bigramstreet}
bigrams_filtered %>% 
  filter(word2 == "street") %>% 
  count(book, word1, sort = TRUE)
## Note that we used `count(book, word1)` not just `count(word1)`. Using just `word1` would have returned a count of all the times a street like Milsom appears in all of the books. It appears 11 times in Northanger Abbey and 5 times in Persuation for a total of 16 times. Adding `book` let's us see a count of word1 by book.
```

We can also treat bigrams as terms for tf-idf.
```{r bigram_tf_idf}
bigrams_tf_idf <- bigrams_united %>% 
  count(book, bigram) %>% 
  bind_tf_idf(bigram, book, n) %>% 
  arrange(desc(tf_idf))
bigrams_tf_idf
```

```{r bigram_tf_idf_plot}

bigrams_tf_idf %>% 
  group_by(book) %>% 
  top_n(15, tf_idf) %>% 
  ungroup() %>% 
  mutate(bigram = reorder(bigram, tf_idf)) %>% 
  ggplot(aes(bigram, tf_idf, fill = book)) +
  geom_col(show.legend = FALSE) +
  labs (x = NULL, y = "tf_idf") +
  facet_wrap(~book, ncol = 2, scales = "free") +
  coord_flip()

## Note that running this code without `mutate(bigram = reorder(bigram, tf_idf))` resulted in a plot where the bigrams were not in descending order.
```

Again we see a lot of proper nouns and names with titles. We also see things like "cried Emma" and "replied Elizabeth" which makes sense since these are main characters who often speak.

### 4.1.3 Using bigrams to provide context in sentiment analysis

Using bigrams for sentiment analysis can add a lot of context. We might see a sentence that says "I am not happy about this" and while the word "happy" is there, we know the sentiment is negative, not positive. Bigrams can help here.

```{r bigram_sentiment}
bigrams_separated %>% 
  filter(word1 == "not") %>% 
  count(word1, word2, sort = TRUE)
```

By looking at bigrams that would change the sentiment of a single word, we can throw out or change the sentiment contribution of that single word.

```{r bigram_afinn1}
sentiments_afinn <- get_sentiments("afinn")

```

```{r not_words}
not_words <- bigrams_separated %>% 
  filter(word1 == "not") %>% 
  inner_join(sentiments_afinn, by = c(word2 = "word")) %>% 
  count(word2, value, sort = TRUE)
not_words
```

Here we see the most common words that are preceded by "not" are the words "like", "help", and "want. We can calculate which words had the biggest impact in the wrong sentiment direction. We can multiply the sentiment score (value) by the number of times it appears (n). 

```{r bigram_afinn_plot}
not_words %>% 
  mutate(contribution = n * value) %>% 
  arrange(desc(abs(contribution))) %>% 
  head(20) %>% 
  mutate(word2 = reorder(word2, contribution)) %>% 
  ggplot(aes(word2, n * value, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Words preceded by \"not\"", 
       y = "Sentiment value * number of occurences") +
  coord_flip()
```

Here we see the top 20 words preceded by "not" that contribute incorrect sentiment. We see that "not like" and "not help" are big contributors!

Let's look at more words that contribute to incorrect sentiment:

```{r bigram_afinn_negation_words}

negation_words <- c("not", "no", "never", "without", "don't", "won't")

negated_words <- bigrams_separated %>% 
  filter(word1 %in% negation_words) %>% 
  inner_join(sentiments_afinn, by = c(word2 = "word")) %>% 
  count(word1, word2, value, sort = TRUE)
negated_words
```

```{r bigram_negated_words_plot}

negated_words %>% 
  mutate(contribution = n * value) %>% 
  arrange(desc(abs(contribution))) %>% 
  head(20) %>% 
  mutate(word2 = reorder(word2, contribution)) %>% 
  ggplot(aes(word2, n * value, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  labs(x = "Words preceded by negating term",
       y = "Sentiment value * number of occurences") +
  coord_flip()
```

### 4.1.4 Visualizing a network of bigrams with ggraph

Another way to look at the relationships between words is to look at a network of multiple words all at once instead of just the top pairs. We can create nodes with the tidy data because the data has three variables: the "from", the "to", and the weight (n). In `igraph` we can use `graph_from_data_frame()`.

```{r bigram_graph}

bigrams_graph <- bigrams_counts %>% 
  filter(n > 20) %>% 
  graph_from_data_frame()
bigrams_graph

set.seed(2017)

## Using `set.seed(2017) will allow the graph below to print out the same for anyone who tries to reproduce the code.

ggraph(bigrams_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

```

In the above graph, we see that there are common nodes of "sir" or "lady" and different character's names branching off.

We can add more adjustments to the graph:

```{r graph_additional_layers}
set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.10, "inches"))

ggraph(bigrams_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), 
                 show.legend = FALSE,
                 arrow = a,
                 end_cap = circle(.01, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

```

### 4.1.5 Visualizing bigrams in other texts

To make it easy to count bigrams and visualize bigrams, we can make a function:

```{r creat_function_count_vis_bigrams}

count_bigrams <- function(dataset) {
  dataset %>% 
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
    separate(bigram, c("word1", "word2"), sep = " ") %>% 
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>% 
    count(word1, word2, sort = TRUE)
}

visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", 
                   length = unit(.15, "inches"))
  
  bigrams %>% 
    graph_from_data_frame() %>% 
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n),
                   show.legend = FALSE,
                   arrow = a) +
    geom_node_point(color = "lightblue", size = 4) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}

```

Now we can use the functions `count_bigrams()` and `visualize_bigrams()` more easily. Let's try it out:

```{r KJV_download, message=FALSE}
## download the text:

kjv <- gutenberg_download(10)

## bigram the text:

kjv_bigrams <- kjv %>% 
  count_bigrams()

## filter out rare combos and numbers

kjv_bigrams %>% 
  filter(n > 40,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>% 
  visualize_bigrams()

```

We see a lot of "thy" and "thou" and "thine". These could be considered stop words.

### 4.2 Counting and correlating pairs of words with the widyr package

Sometimes we're interested in words that aren't right next to one another but tend to co-occur in a document(s), chapter(s), or other grouping. To do this, we need to be able to compare between rows and make our tidy, narrow data, into a wide format. They widyr package can do this.

### 4.2.1 Counting and correlating among sections

Back in Chapter 2, we looked at sections of the book *Pride and Prejudice* for sentiment analysis. We can do something similar to find what words tend to appear within the same section:

```{r austen_section_words}
## we call corpus `austen_books()`, filter for just P&P, add in a new variable called section that is 1 section per ten rows, we don't want to look at section 0 since the actual text of the book starts on row 10, we've unnested tokens and filtered out stop words.

austen_section_words <- austen_books() %>% 
  filter(book == "Pride & Prejudice") %>% 
  mutate(section = row_number() %/% 10) %>% 
    filter(section > 0) %>% 
  unnest_tokens(word, text) %>% 
  filter(!word %in% stop_words$word)

austen_section_words
```

Now we can use the widyr package's `pairwise_count()` to find the words that co-occur within sections. Usage: `pairwise_count(tbl item, feature)`. Where `item` is the thing we're counting and spits out `item1` and `item2`, and where `feature` is the scope of each chunk/co-occurance area.

```{r austen_word_pairs}
word_pairs <- austen_section_words %>% 
  pairwise_count(word, section, sort = TRUE)
word_pairs
```

Here we see the pair Darcy & Elizabeth and Elizabeth & Darcy. Let's find the most common words that co-occur with Darcy:

```{r darcy_pair}
word_pairs %>% 
  filter(item1 == "darcy")
```

### 4.2.2 Pairwise correlation

Looking at the example of Elizabeth & Darcy, we see that they're very common terms in the book, which makes sense, they're the main characters. We could also look at correlation among words, or how often words co-occur compared to how often they occur separately.

The function `pairwise_cor()` in widyr calculates the phi coefficient (equivalent to the Pearson correlation when applied to binary data) between words based on how often they co-occur in the same section.

```{r austen_pairwise_cor}
## first let's filter relatively common words

word_cors <- austen_section_words %>% 
  group_by(word) %>% 
  filter(n() >= 20) %>% 
  pairwise_cor(word, section, sort = TRUE)
word_cors

```

Again, this format is helpful for digging into.
```{r word_cors_pounds}
word_cors %>% 
  filter(item1 == "pounds")
```

```{r other_word_cors, message = FALSE}
word_cors %>% 
  filter(item1 %in% c("elizabeth", "pounds", "married", "pride", "looked")) %>% 
  group_by(item1) %>% 
  top_n(6) %>% 
  ungroup() %>% 
  mutate(item2 = reorder(item2, correlation)) %>% 
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()

## Note that geom_bar's default is `stat = "bin"` where the bar's height is equal the count of whatever x is. Since we want the bars to show the value we've calculated for correlation, we have to tell geom_bar that `stat = "identity"`
```

Next we can visualize the data:

```{r word_cor_viz}
set.seed(2016)

word_cors %>% 
  filter(correlation > .175) %>% 
  graph_from_data_frame() %>% 
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 4) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

Here we see a visualization of words that are at least .175 correlated in terms of appearing in the same 10 line section. Unlike the networks of bigrams, these terms do not have a directional arrow because they are linked by being in the same 10 line section. Again, unlike bigrams, we see words we would expect to see near but perhaps not next to one another such as mother & father, ten & thousand & pounds, and news & brighton.


